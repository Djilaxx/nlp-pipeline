{"cells":[{"metadata":{},"cell_type":"markdown","source":["# ERROR ANALYSIS"]},{"metadata":{},"cell_type":"markdown","source":["Understanding where does our model fail to predict correctly can often be a key to creating the most accurate models."]},{"metadata":{},"cell_type":"markdown","source":["## IMPORTS"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import os, joblib\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn import metrics\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# TORCH\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import transformers\n","from transformers import BertTokenizer"],"execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## UTILS"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["ERROR! Session/line number was not unique in database. History logging moved to new session 1143\n"]}],"source":["class AverageMeter:\n","    \"\"\"\n","    Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import re\n","\n","def remove_URL(text):\n","    url = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url.sub(r'', text)\n","\n","def remove_numbers(text):\n","    text = ''.join([i for i in text if not i.isdigit()])\n","    return text\n","\n","def remove_html(text):\n","    html = re.compile(r'<.*?>')\n","    return html.sub(r'', text)\n","\n","def remove_username(text):\n","    user = re.compile(r'@[A-Za-z0-9_]+')\n","    return user.sub(r'', text)\n","\n","def feature_engineering(text):\n","    text = remove_URL(text)\n","    text = remove_numbers(text)\n","    text = remove_html(text)\n","    text = remove_username(text)\n","    return \" \".join(text.split())"],"execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","    \"\"\"\n","    given a sklearn confusion matrix (cm), make a nice plot\n","\n","    Arguments\n","    ---------\n","    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n","\n","    target_names: given classification classes such as [0, 1, 2]\n","                  the class names, for example: ['high', 'medium', 'low']\n","\n","    title:        the text to display at the top of the matrix\n","\n","    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n","                  see http://matplotlib.org/examples/color/colormaps_reference.html\n","                  plt.get_cmap('jet') or plt.cm.Blues\n","\n","    normalize:    If False, plot the raw numbers\n","                  If True, plot the proportions\n","\n","    Usage\n","    -----\n","    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n","                                                              # sklearn.metrics.confusion_matrix\n","                          normalize    = True,                # show proportions\n","                          target_names = y_labels_vals,       # list of names of the classes\n","                          title        = best_estimator_name) # title of graph\n","\n","    Citiation\n","    ---------\n","    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.show()"],"execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## CONFIG"]},{"metadata":{"trusted":true},"cell_type":"code","source":["config = {\n","    \"TRAIN_PATH\" : \"D:/Documents/GitHub/nlp-pipeline/data/tweet_disaster/train.csv\",\n","    \"TEST_PATH\" : \"D:/Documents/GitHub/nlp-pipeline/data/tweet_disaster/test.csv\",\n","    \"TEXT_VAR\" : \"text\",\n","    \"TARGET_VAR\" : \"target\",\n","    \"DEVICE\" : torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n","    \"MAX_LEN\" : 160,\n","    \"N_CLASS\" : 2,\n","    \"MODEL_NAME\" : \"DISTILBERT\",\n","    \"TASK\" : \"CLASSIFICATION\",\n","    \"EPOCHS\" : 5,\n","    \"LR\" : 0.001\n","}"],"execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## LOADING DATA"]},{"source":["### DATASET CLASS"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class NLP_DATASET(Dataset):\n","    def __init__(self, model_name, task, text, max_len, labels=None, tokenizer=None, feature_eng=None):\n","        self.model_name = model_name\n","        self.task = task\n","        self.text = text\n","        self.labels = labels\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.feature_eng = feature_eng\n","\n","    #RETURN THE LENGHT OF THE DATASET\n","    def __len__(self):\n","        return len(self.text)\n","\n","    #FUNCTION THAT RETURN ONE DATAPOINT (INPUT + LABEL)\n","    def __getitem__(self, index):\n","        # LIST WHERE ONE ROW OF TEXT DATA\n","        text = str(self.text[index])\n","        # USING FEATURE_ENG FUNCTION TO PRE PROCESS TEXT\n","        if self.feature_eng is not None:\n","            text = self.feature_eng(text)\n","        # USING TOKENIZERS ENCODING TO GET TEXT DATA IN CORRECT FORMAT\n","        if self.tokenizer is not None:\n","            inputs = self.tokenizer.encode_plus(\n","                text,\n","                None,\n","                add_special_tokens=True,\n","                max_length=self.max_len,\n","                pad_to_max_length=True,\n","                return_token_type_ids=True,\n","                truncation=True\n","            )\n","\n","        # GETTING ALL DATA NEEDED FOR TRANSFORMERS TRAINING\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        if self.labels is not None:\n","            # LABELS DATA TYPE DEPENDING ON TASK\n","            if self.task == \"CLASSIFICATION\":\n","                labels = torch.tensor(self.labels[index], dtype=torch.long)\n","            elif self.task == \"REGRESSION\":\n","                labels = torch.tensor(self.labels[index], dtype=torch.float32)\n","\n","            # DISTILBERT & ROBERTA DON'T NEED TOKEN_TYPE_IDS\n","            return {\n","                'ids': torch.tensor(ids, dtype=torch.long),\n","                'masks': torch.tensor(mask, dtype=torch.long),\n","                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","                'labels': labels\n","            }\n","        else:\n","            return {\n","                'ids': torch.tensor(ids, dtype=torch.long),\n","                'masks': torch.tensor(mask, dtype=torch.long),\n","                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            }"]},{"source":["### MODEL CLASS"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class DISTILBERT(torch.nn.Module):\n","    def __init__(self, task, model_config_path, n_class=2):\n","        super(DISTILBERT, self).__init__()\n","        self.distilbert = transformers.DistilBertModel.from_pretrained(model_config_path)\n","        self.drop = nn.Dropout(0.3)\n","        if task == \"REGRESSION\":\n","            self.l0 = nn.Linear(768, 1)\n","        elif task == \"CLASSIFICATION\":\n","            self.l0 = nn.Linear(768, n_class)\n","        torch.nn.init.normal_(self.l0.weight, std=0.02)\n","    \n","    def forward(self, ids, mask):\n","        output  = self.distilbert(ids, mask)\n","        hidden_state = output[0]\n","        pooled = hidden_state[:, 0]\n","        out = self.drop(pooled)\n","        out = self.l0(out)\n","        return out"]},{"metadata":{"trusted":true},"cell_type":"code","source":["df = pd.read_csv(config[\"TRAIN_PATH\"])\n","text = df[config[\"TEXT_VAR\"]]\n","target = df[config[\"TARGET_VAR\"]].values\n","\n","# SPLIT A TRAINING & A VALIDATION SET\n","train_text, valid_text, train_labels, valid_labels = train_test_split(text, target, test_size=0.2, random_state=95)"],"execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## TRAINING A MODEL"]},{"source":["### TRAINER"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["#################\n","# TRAINER CLASS #\n","#################\n","class TRAINER:\n","    '''\n","    training_step train the model for one epoch\n","    eval_step evaluate the current model on validation data and output current loss and other evaluation metric\n","    test_step is used to predict on test data\n","    '''\n","    def __init__(self, model, task, device, optimizer=None, criterion=None):\n","        self.model = model\n","        self.task = task\n","        self.device = device\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","\n","    #################\n","    # TRAINING STEP #\n","    #################\n","    def training_step(self, data_loader):\n","        # LOSS AVERAGE\n","        losses = AverageMeter()\n","        # MODEL TO TRAIN MODE\n","        self.model.train()\n","        # TRAINING LOOP\n","        tk0 = tqdm(data_loader, total=len(data_loader))\n","        for _, data in enumerate(tk0):\n","            model_name = self.model.__class__.__name__\n","            # LOADING TEXT TOKENS & LABELS\n","            ids = data[\"ids\"].to(self.device)\n","            masks = data[\"masks\"].to(self.device)\n","            labels = data[\"labels\"].to(self.device)\n","            # BERT REQUIRES TOKEN_TYPE_IDS TOO\n","            if model_name in [\"BERT\"]:\n","                token_type_ids = data[\"token_type_ids\"].to(self.device)\n","                # GETTING PREDICTION FROM MODEL\n","                self.model.zero_grad()\n","                output = self.model(ids=ids, mask=masks, token_type_ids=token_type_ids)\n","\n","            elif model_name in [\"DISTILBERT\", \"ROBERTA\"]:\n","                # GETTING PREDICTION FROM MODEL\n","                self.model.zero_grad()\n","                output = self.model(ids=ids, mask=masks)\n","\n","            # CALCULATE LOSS\n","            loss = self.criterion(output, labels)\n","            # CALCULATE GRADIENTS\n","            loss.backward()\n","            self.optimizer.step()\n","            # UPDATE LOSS\n","            losses.update(loss.item(), ids.size(0))\n","            tk0.set_postfix(loss=losses.avg)\n","\n","    ###################\n","    # VALIDATION STEP #\n","    ###################\n","    def eval_step(self, data_loader, metric):\n","        # LOSS & METRIC AVERAGE\n","        losses = AverageMeter()\n","        metrics_avg = AverageMeter()\n","        # MODEL TO EVAL MODE\n","        self.model.eval()\n","        # VALIDATION LOOP\n","        with torch.no_grad():\n","            tk0 = tqdm(data_loader, total=len(data_loader))\n","            for _, data in enumerate(tk0):\n","                model_name = self.model.__class__.__name__\n","                # LOADING TEXT TOKENS & LABELS\n","                ids = data[\"ids\"].to(self.device)\n","                masks = data[\"masks\"].to(self.device)\n","                labels = data[\"labels\"].to(self.device)\n","                if model_name in [\"BERT\"]:\n","                    token_type_ids = data[\"token_type_ids\"].to(self.device)\n","                    # GETTING PREDICTION FROM MODEL\n","                    output = self.model(ids=ids, mask=masks,\n","                                        token_type_ids=token_type_ids)\n","                elif model_name in [\"DISTILBERT\", \"ROBERTA\"]:\n","                    # GETTING PREDICTION FROM MODEL\n","                    output = self.model(ids=ids, mask=masks)\n","\n","                # CALCULATE LOSS & METRICS\n","                loss = self.criterion(output, labels)\n","\n","                # CHECK FOR REGRESSION VS CLASSIFICATION\n","                if self.task == \"CLASSIFICATION\":\n","                    output = output.argmax(axis=1)\n","                output = output.cpu().detach().numpy()\n","                labels = labels.cpu().detach().numpy()\n","                metric_value = metric(labels, output)\n","\n","                losses.update(loss.item(), ids.size(0))\n","                metrics_avg.update(metric_value.item(), ids.size(0))\n","\n","                tk0.set_postfix(loss=losses.avg)\n","        print(f\"Validation Loss = {losses.avg}\")\n","        return loss, metrics_avg.avg"]},{"source":["## TRAINING LOOP"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# TRAINING DATASET\n","train_ds = NLP_DATASET(\n","    model_name = config[\"MODEL_NAME\"],\n","    task = config[\"TASK\"],\n","    text=train_text,\n","    labels=train_labels,\n","    max_len = config[\"MAX_LEN\"],\n","    tokenizer = tokenizer,\n","    feature_eng = feature_engineering\n",")\n","# TRAINING DATALOADER\n","train_loader = torch.utils.data.DataLoader(\n","    train_ds, \n","    batch_size=32, \n","    shuffle=True, \n","    num_workers=0\n",")\n","# VALIDATION DATASET\n","valid_ds = NLP_DATASET(\n","    model_name = config[\"MODEL_NAME\"],\n","    task = config[\"TASK\"],\n","    text = valid_text,\n","    labels = valid_labels,\n","    max_len = config[\"MAX_LEN\"],\n","    tokenizer = tokenizer,\n","    feature_eng = feature_engineering\n",")\n","# VALIDATION DATALOADER\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_ds, \n","    batch_size=16, \n","    shuffle=True, \n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DISTILBERT(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (l0): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":32}],"source":["model = DISTILBERT(task=config[\"TASK\"],\n","                model_config_path=\"D:/Documents/GitHub/nlp-pipeline/models/DISTILBERT/config/\",\n","                n_class=config[\"N_CLASS\"])\n","model.to(config[\"DEVICE\"])"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=config[\"LR\"])\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","metric = metrics.accuracy_score\n","\n","trainer = TRAINER(model=model,\n","                optimizer=optimizer,\n","                device=config[\"DEVICE\"],\n","                criterion=criterion,\n","                task=config[\"TASK\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(config[\"EPOCHS\"]):\n","    print(f\"Starting epoch number : {epoch}\")\n","    # TRAINING PHASE\n","    print(\"Training the model...\")\n","    trainer.training_step(train_loader)\n","    # VALIDATION PHASE\n","    print(\"Evaluating the model...\")\n","    val_loss, metric_value = trainer.eval_step(valid_loader, metric)\n","    scheduler.step(val_loss)\n","    # METRICS\n","    print(f\"Validation {metric.__name__} = {metric_value}\")"]},{"metadata":{},"cell_type":"markdown","source":["## ERROR ANALYSIS"]},{"metadata":{"trusted":true},"cell_type":"code","source":["detection_threshold = 0.5\n","valid_pred = valid_oof[:, 1] >= detection_threshold\n","l = lambda x : x*1\n","valid_pred = l(valid_pred)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### CONFUSION MATRIX"]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n","cm = confusion_matrix(valid_y, valid_pred)\n","plot_confusion_matrix(cm = cm, normalize = False, target_names = ['0', '1'], title = \"Confusion Matrix\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["valid = pd.DataFrame(valid_x)\n","valid[config[\"TARGET_VAR\"]] = valid_y\n","valid[\"preds\"] = valid_oof[:, 1]\n","valid[\"preds_int\"] = valid_pred\n","valid.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["valid[\"error\"] = abs(valid[config[\"TARGET_VAR\"]] - valid[\"preds\"])\n","valid.head()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's take a look at some of the biggest errors the model made"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_desc = valid.sort_values(by=['error'], ascending=False)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["LOOKING AT FALSE POSITIVE"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_desc[sorted_desc[config[\"TARGET_VAR\"]]==0] [0:20]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["NOW FALSE NEGATIVE"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_desc[sorted_desc[config[\"TARGET_VAR\"]]==1] [0:20]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The inverse let's look at case where the model is correct and very sure about it"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_asc = valid.sort_values(by=['error'], ascending=True)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["TRUE NEGATIVE"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_asc[sorted_asc[config[\"TARGET_VAR\"]]==0] [0:20]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["TRUE POSITIVE"]},{"metadata":{"trusted":true},"cell_type":"code","source":["sorted_asc[sorted_asc[config[\"TARGET_VAR\"]]==1] [0:20]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Analyzing model error depending on variable value"]},{"metadata":{"trusted":true},"cell_type":"code","source":["cm = confusion_matrix(valid[config[\"TARGET_VAR\"]][valid[\"cat16\"] == 0], valid[\"preds_int\"][valid[\"cat16\"] == 0])\n","plot_confusion_matrix(cm = cm, normalize = False, target_names = ['0', '1'], title = \"Confusion Matrix\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["cm = confusion_matrix(valid[config[\"TARGET_VAR\"]][valid[\"cat16\"] == 1], valid[\"preds_int\"][valid[\"cat16\"] == 1])\n","plot_confusion_matrix(cm = cm, normalize = False, target_names = ['0', '1'], title = \"Confusion Matrix\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["cm = confusion_matrix(valid[config[\"TARGET_VAR\"]][valid[\"cat16\"] == 2], valid[\"preds_int\"][valid[\"cat16\"] == 2])\n","plot_confusion_matrix(cm = cm, normalize = False, target_names = ['0', '1'], title = \"Confusion Matrix\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["cm = confusion_matrix(valid[config[\"TARGET_VAR\"]][valid[\"cat16\"] == 3], valid[\"preds_int\"][valid[\"cat16\"] == 3])\n","plot_confusion_matrix(cm = cm, normalize = False, target_names = ['0', '1'], title = \"Confusion Matrix\")"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python377jvsc74a57bd01524824e3f0c83810fc9aefd4d85168626f1cce80efd3b81c1b3d2bfe66cf15a","display_name":"Python 3.7.7 64-bit ('Torch-37': conda)"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.7","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}